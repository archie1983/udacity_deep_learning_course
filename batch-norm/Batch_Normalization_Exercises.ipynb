{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization â€“ Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization is most useful when building deep neural networks. To demonstrate this, we'll create a convolutional neural network with 20 convolutional layers, followed by a fully connected layer. We'll use it to classify handwritten digits in the MNIST dataset, which should be familiar to you by now.\n",
    "\n",
    "This is **not** a good network for classfying MNIST digits. You could create a _much_ simpler network and get _better_ results. However, to give you hands-on experience with batch normalization, we had to make an example that was:\n",
    "1. Complicated enough that training would benefit from batch normalization.\n",
    "2. Simple enough that it would train quickly, since this is meant to be a short exercise just to give you some practice adding batch normalization.\n",
    "3. Simple enough that the architecture would be easy to understand without additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook includes two versions of the network that you can edit. The first uses higher level functions from the `tf.layers` package. The second is the same network, but uses only lower level functions in the `tf.nn` package.\n",
    "\n",
    "1. [Batch Normalization with `tf.layers.batch_normalization`](#example_1)\n",
    "2. [Batch Normalization with `tf.nn.batch_normalization`](#example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads TensorFlow, downloads the MNIST dataset if necessary, and loads it into an object named `mnist`. You'll need to run this cell before running anything else in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization using `tf.layers.batch_normalization`<a id=\"example_1\"></a>\n",
    "\n",
    "This version of the network uses `tf.layers` for almost everything, and expects you to implement batch normalization using [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create fully connected layers in our network. We'll create them with the specified number of neurons and a ReLU activation function.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def fully_connected(prev_layer, num_units):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following function to create convolutional layers in our network. They are very basic: we're always using a 3x3 kernel, ReLU activation functions, strides of 1x1 on layers with odd depths, and strides of 2x2 on layers with even depths. We aren't bothering with pooling layers at all in this network.\n",
    "\n",
    "This version of the function does not include batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def conv_layer(prev_layer, layer_depth):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the following cell**, along with the earlier cells (to load the dataset and define the necessary functions). \n",
    "\n",
    "This cell builds the network **without** batch normalization, then trains it on the MNIST dataset. It displays loss and accuracy data periodically while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69060, Validation accuracy: 0.11000\n",
      "Batch: 25: Training loss: 0.35825, Training accuracy: 0.09375\n",
      "Batch: 50: Training loss: 0.32565, Training accuracy: 0.09375\n",
      "Batch: 75: Training loss: 0.32712, Training accuracy: 0.06250\n",
      "Batch: 100: Validation loss: 0.32510, Validation accuracy: 0.11260\n",
      "Batch: 125: Training loss: 0.32646, Training accuracy: 0.03125\n",
      "Batch: 150: Training loss: 0.32563, Training accuracy: 0.10938\n",
      "Batch: 175: Training loss: 0.32313, Training accuracy: 0.09375\n",
      "Batch: 200: Validation loss: 0.32612, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.32526, Training accuracy: 0.09375\n",
      "Batch: 250: Training loss: 0.32680, Training accuracy: 0.07812\n",
      "Batch: 275: Training loss: 0.32330, Training accuracy: 0.12500\n",
      "Batch: 300: Validation loss: 0.32580, Validation accuracy: 0.09240\n",
      "Batch: 325: Training loss: 0.32323, Training accuracy: 0.15625\n",
      "Batch: 350: Training loss: 0.32320, Training accuracy: 0.12500\n",
      "Batch: 375: Training loss: 0.32707, Training accuracy: 0.07812\n",
      "Batch: 400: Validation loss: 0.32492, Validation accuracy: 0.11260\n",
      "Batch: 425: Training loss: 0.32555, Training accuracy: 0.09375\n",
      "Batch: 450: Training loss: 0.32604, Training accuracy: 0.10938\n",
      "Batch: 475: Training loss: 0.32651, Training accuracy: 0.04688\n",
      "Batch: 500: Validation loss: 0.32533, Validation accuracy: 0.10020\n",
      "Batch: 525: Training loss: 0.32875, Training accuracy: 0.01562\n",
      "Batch: 550: Training loss: 0.32387, Training accuracy: 0.20312\n",
      "Batch: 575: Training loss: 0.32311, Training accuracy: 0.10938\n",
      "Batch: 600: Validation loss: 0.32545, Validation accuracy: 0.09760\n",
      "Batch: 625: Training loss: 0.32433, Training accuracy: 0.14062\n",
      "Batch: 650: Training loss: 0.32774, Training accuracy: 0.07812\n",
      "Batch: 675: Training loss: 0.32560, Training accuracy: 0.12500\n",
      "Batch: 700: Validation loss: 0.32527, Validation accuracy: 0.09860\n",
      "Batch: 725: Training loss: 0.32524, Training accuracy: 0.09375\n",
      "Batch: 750: Training loss: 0.32528, Training accuracy: 0.14062\n",
      "Batch: 775: Training loss: 0.32547, Training accuracy: 0.12500\n",
      "Final validation accuracy: 0.11000\n",
      "Final test accuracy: 0.10280\n",
      "Accuracy on 100 samples: 0.15\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DO NOT MODIFY THIS CELL\n",
    "\"\"\"\n",
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]]})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this many layers, it's going to take a lot of iterations for this network to learn. By the time you're done training these 800 batches, your final test and validation accuracies probably won't be much better than 10%. (It will be different each time, but will most likely be less than 15%.)\n",
    "\n",
    "Using batch normalization, you'll be able to train this same network to over 90% in that same number of batches.\n",
    "\n",
    "\n",
    "# Add batch normalization\n",
    "\n",
    "We've copied the previous three cells to get you started. **Edit these cells** to add batch normalization to the network. For this exercise, you should use [`tf.layers.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization) to handle most of the math, but you'll need to make a few other changes to your network to integrate batch normalization. You may want to refer back to the lesson notebook to remind yourself of important things, like how your graph operations need to know whether or not you are performing training or inference. \n",
    "\n",
    "If you get stuck, you can check out the `Batch_Normalization_Solutions` notebook to see how we did things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `fully_connected` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AE: added parameter <is_training>, which should be a tf.bool placeholder\n",
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    #layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    # AE: We are going to have an ordinary dense (or fully connected layer), then we need to apply batch normalisation\n",
    "    # AE: BEFORE we apply activation function, so we'll make sure, that activation function is None and then we'll apply\n",
    "    # AE: activation at the end.\n",
    "    layer = tf.layers.dense(prev_layer, num_units, activation=None, use_bias=False)\n",
    "    layer = tf.layers.batch_normalization(inputs=layer, training=is_training)\n",
    "    layer = tf.nn.relu(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `conv_layer` to add batch normalization to the convolutional layers it creates. Feel free to change the function's parameters if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AE: added parameter <is_training> which should be a tf.bool placeholder\n",
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    #conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "    # AE: Very similar as above- adding batch normalisation to the convolutional layer BEFORE activation, then adding activation.\n",
    "    conv_layer = tf.layers.conv2d(prev_layer, layer_depth*4, 3, strides, 'same', activation=None, use_bias=False)\n",
    "    conv_layer = tf.layers.batch_normalization(inputs=conv_layer, training=is_training)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return conv_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Edit the `train` function to support batch normalization. You'll need to make sure the network knows whether or not it is training, and you'll need to make sure it updates and uses its population statistics correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69097, Validation accuracy: 0.08680\n",
      "Batch: 25: Training loss: 0.33733, Training accuracy: 0.28125\n",
      "Batch: 50: Training loss: 0.26345, Training accuracy: 0.46875\n",
      "Batch: 75: Training loss: 0.21008, Training accuracy: 0.53125\n",
      "Batch: 100: Validation loss: 0.37897, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.08809, Training accuracy: 0.89062\n",
      "Batch: 150: Training loss: 0.06718, Training accuracy: 0.92188\n",
      "Batch: 175: Training loss: 0.05523, Training accuracy: 0.95312\n",
      "Batch: 200: Validation loss: 0.36617, Validation accuracy: 0.09220\n",
      "Batch: 225: Training loss: 0.07558, Training accuracy: 0.90625\n",
      "Batch: 250: Training loss: 0.03752, Training accuracy: 0.98438\n",
      "Batch: 275: Training loss: 0.06072, Training accuracy: 0.92188\n",
      "Batch: 300: Validation loss: 0.21706, Validation accuracy: 0.59800\n",
      "Batch: 325: Training loss: 0.02993, Training accuracy: 0.98438\n",
      "Batch: 350: Training loss: 0.01218, Training accuracy: 1.00000\n",
      "Batch: 375: Training loss: 0.03881, Training accuracy: 0.93750\n",
      "Batch: 400: Validation loss: 0.07263, Validation accuracy: 0.89480\n",
      "Batch: 425: Training loss: 0.04762, Training accuracy: 0.93750\n",
      "Batch: 450: Training loss: 0.02045, Training accuracy: 0.96875\n",
      "Batch: 475: Training loss: 0.03042, Training accuracy: 0.93750\n",
      "Batch: 500: Validation loss: 0.04122, Validation accuracy: 0.94320\n",
      "Batch: 525: Training loss: 0.03206, Training accuracy: 0.92188\n",
      "Batch: 550: Training loss: 0.04799, Training accuracy: 0.92188\n",
      "Batch: 575: Training loss: 0.00972, Training accuracy: 0.98438\n",
      "Batch: 600: Validation loss: 0.02254, Validation accuracy: 0.96900\n",
      "Batch: 625: Training loss: 0.01655, Training accuracy: 0.98438\n",
      "Batch: 650: Training loss: 0.01429, Training accuracy: 0.98438\n",
      "Batch: 675: Training loss: 0.01710, Training accuracy: 0.98438\n",
      "Batch: 700: Validation loss: 0.12442, Validation accuracy: 0.83300\n",
      "Batch: 725: Training loss: 0.01652, Training accuracy: 0.95312\n",
      "Batch: 750: Training loss: 0.01935, Training accuracy: 0.96875\n",
      "Batch: 775: Training loss: 0.03252, Training accuracy: 0.96875\n",
      "Final validation accuracy: 0.96640\n",
      "Final test accuracy: 0.96680\n",
      "Accuracy on 100 samples: 1.0\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # AE: When we do infering (predictions), then we will want to use the previously (during training)\n",
    "    # AE: calculated population statistics about the samples accross all batches. For that to happen\n",
    "    # AE: we will need our batch normalisation layers to know that we're infering and not training.\n",
    "    # AE: That's why we will have to pass this placeholder to the batch normalisation function, \n",
    "    # AE: which we can easily change between different session runs through the feed_dict parameter.\n",
    "    # AE: This placeholder will be passed to \"fully_connected\" and \"conv_layer\" functions and there\n",
    "    # AE: it will be used in constructing the batch normalisation layers.\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training=is_training) # AE: added the is_training=is_training parameter\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training=is_training) # AE: added the is_training=is_training parameter\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    #train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "\n",
    "    # AE: Now that we're doing batch normalisation, we need to update population mean and variance on every batch,\n",
    "    # AE: because batch normalisation calculates batch mean and batch variance for every batch, but that all needs\n",
    "    # AE: to be accumulated so that when we do our predictions, we use the actual mean and variance of the whole\n",
    "    # AE: population, rather than just the last batch. This process doesn't happen automatically (at least not yet)\n",
    "    # AE: so we need to force a dependency on the optimizer to sort the population statistics before it runs the next\n",
    "    # AE: batch.\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            # AE: added the is_training: True parameter\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                # AE: added the is_training: False parameter\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False})\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                # AE: added the is_training: True parameter. Here we are only checking the accuracy of the current batch\n",
    "                # AE: so the whole population statistics are not really required, hence the parameter value of True. This\n",
    "                # AE: may be giving \"an unfair\" advantage to the training accuracy estimation, because if we used full\n",
    "                # AE: population statistics here, we probably would get worse results towards the beginning and better\n",
    "                # AE: results as the training progresses, but this way we get the value of exactly how good the current\n",
    "                # AE: batch was trained.\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        # AE: added the is_training: False parameter\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        # AE: added the is_training: False parameter\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            # AE: added the is_training: False parameter\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With batch normalization, you should now get an accuracy over 90%. Notice also the last line of the output: `Accuracy on 100 samples`. If this value is low while everything else looks good, that means you did not implement batch normalization correctly. Specifically, it means you either did not calculate the population mean and variance while training, or you are not using those values during inference.\n",
    "\n",
    "# Batch Normalization using `tf.nn.batch_normalization`<a id=\"example_2\"></a>\n",
    "\n",
    "Most of the time you will be able to use higher level functions exclusively, but sometimes you may want to work at a lower level. For example, if you ever want to implement a new feature â€“ something new enough that TensorFlow does not already include a high-level implementation of it, like batch normalization in an LSTM â€“ then you may need to know these sorts of things.\n",
    "\n",
    "This version of the network uses `tf.nn` for almost everything, and expects you to implement batch normalization using [`tf.nn.batch_normalization`](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization).\n",
    "\n",
    "**Optional TODO:** You can run the next three cells before you edit them just to see how the network performs without batch normalization. However, the results should be pretty much the same as you saw with the previous example before you added batch normalization. \n",
    "\n",
    "**TODO:** Modify `fully_connected` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** For convenience, we continue to use `tf.layers.dense` for the `fully_connected` layer. By this point in the class, you should have no problem replacing that with matrix operations between the `prev_layer` and explicit weights and biases variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(prev_layer, num_units, is_training):\n",
    "    \"\"\"\n",
    "    Create a fully connectd layer with the given layer as input and the given number of neurons.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param num_units: int\n",
    "        The size of the layer. That is, the number of units, nodes, or neurons.\n",
    "    :returns Tensor\n",
    "        A new fully connected layer\n",
    "    \"\"\"\n",
    "    #layer = tf.layers.dense(prev_layer, num_units, activation=tf.nn.relu)\n",
    "    layer = tf.layers.dense(prev_layer, num_units, use_bias=False, activation=None)\n",
    "\n",
    "    # AE: To do batch normalisation ourselves, we'll need to use tf.nn.batch_normalization and pass several parameters to it\n",
    "    # AE: that will have to be able to change. They all can be tf.Variable, because they are only given initial value at first\n",
    "    # AE: and then are modified by the process. Mean and variance will have to come from the tf.nn.moments function if training.\n",
    "    # AE: The shapes of beta, gamma, mean and variance will be a vector, but the size of the vector will change from layer to \n",
    "    # AE: layer, but that doesn't matter if we construct them as local variables, because the instances of these tf.Variable\n",
    "    # AE: items will still be trainable.\n",
    "    # AE:\n",
    "    # AE: Mean and variance of course should not be trainable, we fill them in ourselves from the batches according to the \n",
    "    # AE: formulas given in the \"lesson\" jupyter notebook.\n",
    "    mean = tf.Variable(tf.zeros(shape=[num_units], dtype=tf.float32), trainable=False)\n",
    "    variance = tf.Variable(tf.ones(shape=[num_units], dtype=tf.float32), trainable=False)\n",
    "\n",
    "    # AE: Beta and gamma have to be trainable and will initially be zeroes and ones, to leave no impact at first.\n",
    "    # AE: because the formula is: xi * gamma + beta (see the \"Lesson\" jupyter notebook).\n",
    "    beta = tf.Variable(tf.zeros(shape=[num_units], dtype=tf.float32))\n",
    "    gamma = tf.Variable(tf.ones(shape=[num_units], dtype=tf.float32))\n",
    "\n",
    "    # AE: Epsilon is just a little constant to avoid a possibility of dividing by zero and also to expand the batch variance\n",
    "    # AE: a little bit, because the total population variance is always higher than the batch variance and we'll get better\n",
    "    # AE: results if the value here will be closer to the population variance, than the actual batch variance.\n",
    "    epsilon = 0.001\n",
    "\n",
    "    # AE: Now if we're training, we'll be calculating batch mean and variance and we'll need to update the population mean \n",
    "    # AE: and variance with those values.\n",
    "    # AE:\n",
    "    # AE: But if we're infering, then we'll be using population mean and variance for the calculations and will not be updating\n",
    "    # AE: them.\n",
    "\n",
    "    def when_training():\n",
    "        batch_mean, batch_variance = tf.nn.moments(x=layer, axes=[0], keep_dims=False)\n",
    "        \n",
    "        # AE: We calculate the updated population mean and variance here and assign it, but we will need to make sure\n",
    "        # AE: that this calculation is done before normalisation, so we'll need to call tf.assign here and create an\n",
    "        # AE: artificial dependency on the reference returned from tf.assign (see below).\n",
    "        # AE: \n",
    "        # AE: When updating, we'll yse a decay variable to make sure that the impact of the newly calculated mean and variance\n",
    "        # AE: only affects the total population values gradually and doesn't make them \"jump\".\n",
    "        decay = 0.99\n",
    "        tmp_new_mean = tf.assign(mean, mean * decay + batch_mean * (1 - decay))\n",
    "        tmp_new_variance = tf.assign(variance, variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        # AE: We need to make sure that the batch_normalisation is calculated AFTER we've updated population mean and\n",
    "        # AE: variance, so we need to make a dependency here on the temporary variables that we created earlier.\n",
    "        with tf.control_dependencies([tmp_new_mean, tmp_new_variance]):\n",
    "            # AE: When training, we want to normalise batches with the current batch mean and variance,\n",
    "            # AE: hence we use batch_variance and batch_mean\n",
    "            layer_local = tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "            return layer_local\n",
    "\n",
    "    def when_infering():\n",
    "        # AE: When infering, we want to use global population mean and variance,\n",
    "        # AE: hence we use variance and mean which pertain to the population values.\n",
    "        layer_local = tf.nn.batch_normalization(layer, mean, variance, beta, gamma, epsilon)\n",
    "        return layer_local\n",
    "    \n",
    "    layer = tf.cond(is_training, when_training, when_infering)\n",
    "    \n",
    "    # AE: At the end of course we want the activation function on the layer that we will be using in any case.\n",
    "    layer = tf.nn.relu(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Modify `conv_layer` to add batch normalization to the fully connected layers it creates. Feel free to change the function's parameters if it helps.\n",
    "\n",
    "**Note:** Unlike in the previous example that used `tf.layers`, adding batch normalization to these convolutional layers _does_ require some slight differences to what you did in `fully_connected`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(prev_layer, layer_depth, is_training):\n",
    "    \"\"\"\n",
    "    Create a convolutional layer with the given layer as input.\n",
    "    \n",
    "    :param prev_layer: Tensor\n",
    "        The Tensor that acts as input into this layer\n",
    "    :param layer_depth: int\n",
    "        We'll set the strides and number of feature maps based on the layer's depth in the network.\n",
    "        This is *not* a good way to make a CNN, but it helps us create this example with very little code.\n",
    "    :returns Tensor\n",
    "        A new convolutional layer\n",
    "    \"\"\"\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "\n",
    "    in_channels = prev_layer.get_shape().as_list()[3]\n",
    "    out_channels = layer_depth*4\n",
    "    \n",
    "    # AE: Not entirely sure what happens here, but we will use \"weights\" as the \"filter\" parameter to tf.nn.conv2d function\n",
    "    # AE: when creating the convolutional layer. Looks like the 3 x 3 is the size of the filter, the out_channels is the value\n",
    "    # AE: that changes according to the depth of this layer (yeah, funny, I know, but basically this is the number of filters\n",
    "    # AE: to use for this layer) and the the in_channels seems to be a dimension of the 4D input tensor. This in_channels to \n",
    "    # AE: my understanding should correspond to the number of colour channels that we use in a picture (so 3 for RGB and 1 for \n",
    "    # AE: B/W), so in this case should be 1.\n",
    "    # AE:\n",
    "    # AE: Here is what's in the TF documentation:\n",
    "    # (...) Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape \n",
    "    # [filter_height, filter_width, in_channels, out_channels], this op performs the following: (...)\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([3, 3, in_channels, out_channels], stddev=0.05))\n",
    "\n",
    "    bias = tf.Variable(tf.zeros(out_channels))\n",
    "\n",
    "    #conv_layer = tf.nn.conv2d(prev_layer, weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "    c_layer = tf.nn.conv2d(input=prev_layer, filter=weights, strides=[1,strides, strides, 1], padding='SAME')\n",
    "    \n",
    "    # AE: Same as in \"fully_connected\" function (see comments above) we will need several variables to implement batch\n",
    "    # AE: normalisation:\n",
    "    #image_height = prev_layer.get_shape().as_list()[1]\n",
    "    #image_width = prev_layer.get_shape().as_list()[2]\n",
    "\n",
    "    # AE: A significant difference here is that we don't want to calculate mean and variance per node in the layer, but rather\n",
    "    # AE: per filter. And since we will have <out_channels> number of filters (each with the dimensions of: [3, 3, in_channels]).\n",
    "    # AE: So only <out_channels> values in each vector.\n",
    "    mean = tf.Variable(tf.zeros(shape=[out_channels], dtype=tf.float32), trainable=False)\n",
    "    variance = tf.Variable(tf.ones(shape=[out_channels], dtype=tf.float32), trainable=False)\n",
    "    beta = tf.Variable(tf.zeros(shape=[out_channels], dtype=tf.float32))\n",
    "    gamma = tf.Variable(tf.ones(shape=[out_channels], dtype=tf.float32))\n",
    "    epsilon = 0.001\n",
    "    \n",
    "    # AE: And just like above in the \"fully_connected\" function, we perform batch normalisation in two scenarios:\n",
    "    # AE: when training and when infering. And again we use a decay value and again we set up dependency for the\n",
    "    # AE: population mean and variance updates.\n",
    "\n",
    "    def when_training():\n",
    "        # AE: An important moment here. Since we only want to calculate variance and mean (i.e. moments)\n",
    "        # AE: for the feature maps and not the whole layer, we need to pass the special value for axes \n",
    "        # AE: and keep_dims parameters as per recommendation in tf.nn.moments documnetation below :\n",
    "        # (...) for so-called \"global normalization\", used with convolutional filters with shape [batch, height, width, depth], \n",
    "        # pass axes=[0, 1, 2]. (...)\n",
    "        # AE:\n",
    "        # AE: And of course, the dimensionality will not be the same anymore as the input. The input is a 4D tensor, the output\n",
    "        # AE: now needs to be a 1D tensor.\n",
    "        batch_mean, batch_variance = tf.nn.moments(x=c_layer, axes=[0, 1, 2], keep_dims=False)\n",
    "\n",
    "        decay = 0.99\n",
    "        tmp_new_mean = tf.assign(mean, mean * decay + batch_mean * (1 - decay))\n",
    "        tmp_new_variance = tf.assign(variance, variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([tmp_new_mean, tmp_new_variance]):\n",
    "            c_layer_local = tf.nn.batch_normalization(c_layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "            return c_layer_local\n",
    "\n",
    "    def when_infering():\n",
    "        c_layer_local = tf.nn.batch_normalization(c_layer, mean, variance, beta, gamma, epsilon)\n",
    "        return c_layer_local\n",
    "\n",
    "    c_layer = tf.cond(is_training, when_training, when_infering)\n",
    "    \n",
    "    c_layer = tf.nn.bias_add(c_layer, bias)\n",
    "    c_layer = tf.nn.relu(c_layer)\n",
    "\n",
    "    return c_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Edit the `train` function to support batch normalization. You'll need to make sure the network knows whether or not it is training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0: Validation loss: 0.69117, Validation accuracy: 0.10700\n",
      "Batch: 25: Training loss: 0.59189, Training accuracy: 0.10938\n",
      "Batch: 50: Training loss: 0.47506, Training accuracy: 0.06250\n",
      "Batch: 75: Training loss: 0.40561, Training accuracy: 0.03125\n",
      "Batch: 100: Validation loss: 0.36587, Validation accuracy: 0.08680\n",
      "Batch: 125: Training loss: 0.35462, Training accuracy: 0.06250\n",
      "Batch: 150: Training loss: 0.35443, Training accuracy: 0.04688\n",
      "Batch: 175: Training loss: 0.36261, Training accuracy: 0.03125\n",
      "Batch: 200: Validation loss: 0.36426, Validation accuracy: 0.09860\n",
      "Batch: 225: Training loss: 0.37292, Training accuracy: 0.09375\n",
      "Batch: 250: Training loss: 0.34525, Training accuracy: 0.06250\n",
      "Batch: 275: Training loss: 0.40170, Training accuracy: 0.15625\n",
      "Batch: 300: Validation loss: 0.39395, Validation accuracy: 0.13620\n",
      "Batch: 325: Training loss: 0.41931, Training accuracy: 0.12500\n",
      "Batch: 350: Training loss: 0.39284, Training accuracy: 0.21875\n",
      "Batch: 375: Training loss: 0.40014, Training accuracy: 0.32812\n",
      "Batch: 400: Validation loss: 0.28581, Validation accuracy: 0.47320\n",
      "Batch: 425: Training loss: 0.26765, Training accuracy: 0.53125\n",
      "Batch: 450: Training loss: 0.17902, Training accuracy: 0.78125\n",
      "Batch: 475: Training loss: 0.09587, Training accuracy: 0.84375\n",
      "Batch: 500: Validation loss: 0.15708, Validation accuracy: 0.78320\n",
      "Batch: 525: Training loss: 0.08495, Training accuracy: 0.87500\n",
      "Batch: 550: Training loss: 0.04836, Training accuracy: 0.93750\n",
      "Batch: 575: Training loss: 0.10580, Training accuracy: 0.87500\n",
      "Batch: 600: Validation loss: 0.07383, Validation accuracy: 0.91320\n",
      "Batch: 625: Training loss: 0.02114, Training accuracy: 0.96875\n",
      "Batch: 650: Training loss: 0.01738, Training accuracy: 0.95312\n",
      "Batch: 675: Training loss: 0.04820, Training accuracy: 0.93750\n",
      "Batch: 700: Validation loss: 0.03510, Validation accuracy: 0.95680\n",
      "Batch: 725: Training loss: 0.06822, Training accuracy: 0.90625\n",
      "Batch: 750: Training loss: 0.04256, Training accuracy: 0.95312\n",
      "Batch: 775: Training loss: 0.00886, Training accuracy: 0.98438\n",
      "Final validation accuracy: 0.97520\n",
      "Final test accuracy: 0.97650\n",
      "Accuracy on 100 samples: 0.99\n"
     ]
    }
   ],
   "source": [
    "def train(num_batches, batch_size, learning_rate):\n",
    "    # Build placeholders for the input samples and labels \n",
    "    inputs = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    labels = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # AE: again same as with the previous train function, we need to know if we're training or not:\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "    \n",
    "    # Feed the inputs into a series of 20 convolutional layers \n",
    "    layer = inputs\n",
    "    for layer_i in range(1, 20):\n",
    "        layer = conv_layer(layer, layer_i, is_training)\n",
    "\n",
    "    # Flatten the output from the convolutional layers \n",
    "    orig_shape = layer.get_shape().as_list()\n",
    "    layer = tf.reshape(layer, shape=[-1, orig_shape[1] * orig_shape[2] * orig_shape[3]])\n",
    "\n",
    "    # Add one fully connected layer\n",
    "    layer = fully_connected(layer, 100, is_training)\n",
    "\n",
    "    # Create the output layer with 1 node for each \n",
    "    logits = tf.layers.dense(layer, 10)\n",
    "    \n",
    "    # Define loss and training operations\n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_opt = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    # Create operations to test accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Train and test the network\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for batch_i in range(num_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # train this batch\n",
    "            #sess.run(train_opt, {inputs: batch_xs, labels: batch_ys})\n",
    "            sess.run(train_opt, {inputs: batch_xs, labels: batch_ys, is_training: True})\n",
    "            \n",
    "            # Periodically check the validation or training loss and accuracy\n",
    "            if batch_i % 100 == 0:\n",
    "                #loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: mnist.validation.images,\n",
    "                                                              labels: mnist.validation.labels,\n",
    "                                                              is_training: False}) # AE: added this line\n",
    "                print('Batch: {:>2}: Validation loss: {:>3.5f}, Validation accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "            elif batch_i % 25 == 0:\n",
    "                #loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys})\n",
    "                loss, acc = sess.run([model_loss, accuracy], {inputs: batch_xs, labels: batch_ys, is_training: False})\n",
    "                print('Batch: {:>2}: Training loss: {:>3.5f}, Training accuracy: {:>3.5f}'.format(batch_i, loss, acc))\n",
    "\n",
    "        # At the end, score the final accuracy for both the validation and test sets\n",
    "        \n",
    "        #acc = sess.run(accuracy, {inputs: mnist.validation.images, labels: mnist.validation.labels})\n",
    "        acc = sess.run(accuracy, {inputs: mnist.validation.images,\n",
    "                                  labels: mnist.validation.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final validation accuracy: {:>3.5f}'.format(acc))\n",
    "        #acc = sess.run(accuracy, {inputs: mnist.test.images, labels: mnist.test.labels})\n",
    "        acc = sess.run(accuracy, {inputs: mnist.test.images,\n",
    "                                  labels: mnist.test.labels,\n",
    "                                  is_training: False})\n",
    "        print('Final test accuracy: {:>3.5f}'.format(acc))\n",
    "        \n",
    "        # Score the first 100 test images individually. This won't work if batch normalization isn't implemented correctly.\n",
    "        correct = 0\n",
    "        for i in range(100):\n",
    "            #correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]], labels: [mnist.test.labels[i]]})\n",
    "            correct += sess.run(accuracy,feed_dict={inputs: [mnist.test.images[i]],\n",
    "                                                    labels: [mnist.test.labels[i]],\n",
    "                                                    is_training: False})\n",
    "\n",
    "        print(\"Accuracy on 100 samples:\", correct/100)\n",
    "\n",
    "\n",
    "num_batches = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(num_batches, batch_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the model with batch normalization should reach an accuracy over 90%. There are plenty of details that can go wrong when implementing at this low level, so if you got it working - great job! If not, do not worry, just look at the `Batch_Normalization_Solutions` notebook to see what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
